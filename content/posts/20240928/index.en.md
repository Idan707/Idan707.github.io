<!-- ---
title: "Scaling Machine Learning with Rust"
subtitle: "My Journey to High Performance"
date: 2024-09-28T13:57:45+03:00
draft: false

tags: ["rust", "machine-learning", "optimization", "scaling", "data science"]
categories: ["Rust", "Machine Learning", "Engineering"]
---

In the past few months, I faced a challenge: creating a service to serve a machine learning model at high scale with fast response times. My choice of language was Rust ðŸ¦€, and after tests seemed fine, live results showed high CPU consumption and failure to hit the needed scale. Needless to say, this was far from my expectations, so I started my investigation ðŸ•µðŸ»â€â™‚ï¸.

## Profiling the Code

My first go-to solution was to **profile my code**. Results showed the Logistic Regression `predict` function was **single-threaded** and **CPU-heavy**. I initially tried parallelizing it, assuming that would improve performanceâ€”but it didnâ€™t. The bottleneck persisted.

Frustrated, I asked a colleague for a fresh perspective. After some thought, his advice was to **measure execution time** to get a clearer picture of what was happening. Thatâ€™s when it clickedâ€”his advice reminded me that **CPU consumption and execution time are linked**.

```rust
// Original (inefficient) predict function
INTERCEPT: f32 = ... ;
COEFFICIENTS: CsVec<f32> = ... ;

fn predict(features:&CsVec<f32>) -> f32 { 
    COEFFICIENTS.dot(&features) + INTERCEPT
}
```

## The Problem with the Predict Function

The `predict` function in Logistic Regression is pretty straightforwardâ€”compute the dot product of feature and coefficient vectors, then add the intercept. 

The real challenge boiled down to two optimizations:

1. How to store feature values and coefficients.
2. How to calculate the dot product.

What I initially thought was a no-brainer (using sparse matrices with built-in `.dot` functions) turned out to be the bottleneck. A simple test showed that combining a **dense vector for coefficients** and **sparse vector for features** reduced computation latency from **20 ms to less than 1 ms**, and CPU consumption dropped from **80% to below 5%**.

## Why This Works

You might wonder why this approach worked. The answer lies in Rustâ€™s **dense vectors** allowing `O(1)` access to coefficients, while **sparse vectors** efficiently keep feature values and indices. This resulted in a major performance boostâ€”cutting latency and CPU usage dramatically.

```rust
// New (efficient) predict function
INTERCEPT: f32 = ... ;
COEFFICIENTS: Vec<f32> = ... ;

fn predict(features:&CsVec<f32>) -> f32 { 
    features.indices().iter().map(|&i| COEFFICIENTS[i] * features.get(i).unwrap()).sum::<f32>() + INTERCEPT
}
```

## Key Takeaways:

1. **Donâ€™t hesitate to ask for help**â€”fresh eyes can spot what you miss.
2. **Always profile and measure execution time early**â€”it saves time and reveals the real issues.
3. **Donâ€™t blindly trust built-in functions**. While theyâ€™re good general solutions, custom implementations may be better suited for your needs. -->
